{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4588fa7f",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Name: Pola Gnana Shekar\n",
    "\n",
    "Roll No: 21CS10052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02eca1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa07cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('../../dataset/decision-tree.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca04d717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12340\\2441973923.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;31m# Perform reduced error pruning on the tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m \u001b[0mreduced_error_pruning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Outcome'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;31m# Calculate train and test accuracies at different depths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12340\\2441973923.py\u001b[0m in \u001b[0;36mreduced_error_pruning\u001b[1;34m(node, data, target)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;31m# Prune children nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild_node\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mreduced_error_pruning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;31m# Calculate error without pruning (using the original unpruned tree)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12340\\2441973923.py\u001b[0m in \u001b[0;36mreduced_error_pruning\u001b[1;34m(node, data, target)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;31m# Calculate error without pruning (using the original unpruned tree)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[0mpredictions_before_pruning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_point\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata_point\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[0merror_before_pruning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_before_pruning\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12340\\2441973923.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;31m# Calculate error without pruning (using the original unpruned tree)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[0mpredictions_before_pruning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_point\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata_point\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[0merror_before_pruning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_before_pruning\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12340\\2441973923.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(node, data_point)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_point\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = '../../dataset/decision-tree.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Create an imputer instance\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Impute missing values in the entire dataset\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "def normalize_data(data):\n",
    "    # Exclude the \"Outcome\" column from normalization\n",
    "    features = data.drop(columns=['Outcome'])\n",
    "    \n",
    "    # Calculate the minimum and maximum values for each feature\n",
    "    min_vals = features.min()\n",
    "    max_vals = features.max()\n",
    "    \n",
    "    # Perform normalization for each feature\n",
    "    normalized_features = (features - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "    # Combine the normalized features with the \"Outcome\" column\n",
    "    normalized_data = pd.concat([normalized_features, data['Outcome']], axis=1)\n",
    "    \n",
    "    return normalized_data\n",
    "\n",
    "# Normalize the data before splitting\n",
    "data = normalize_data(data)\n",
    "\n",
    "# Split the data into training and testing sets using scikit-learn\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Implement the ID3 Decision Tree algorithm\n",
    "class TreeNode:\n",
    "    def __init__(self, attribute=None, label=None):\n",
    "        self.attribute = attribute\n",
    "        self.label = label\n",
    "        self.children = {}\n",
    "\n",
    "def entropy(target):\n",
    "    # Calculate the entropy of a target variable\n",
    "    unique_labels, counts = np.unique(target, return_counts=True)\n",
    "    probabilities = counts / len(target)\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "def information_gain(data, target, attribute):\n",
    "    total_entropy = entropy(target)\n",
    "    values, counts = np.unique(data[attribute], return_counts=True)\n",
    "    probabilities = counts / len(data)\n",
    "\n",
    "    weighted_entropy = np.sum(probabilities * [entropy(target[data[attribute] == value]) for value in values])\n",
    "\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "def id3_decision_tree(data, attributes, target_attribute, depth=0, max_depth=11):\n",
    "    unique_classes = data[target_attribute].unique()\n",
    "    \n",
    "    # Create a tree node\n",
    "    if len(unique_classes) == 1:\n",
    "        return TreeNode(label=unique_classes[0])\n",
    "\n",
    "    if len(attributes) == 0 or depth >= max_depth:\n",
    "        majority_class = data[target_attribute].mode()[0]\n",
    "        return TreeNode(label=majority_class)\n",
    "\n",
    "    # Find the best attribute to split on\n",
    "    best_attribute = select_best_attribute(data, attributes, target_attribute)\n",
    "\n",
    "    root = TreeNode(attribute=best_attribute)\n",
    "\n",
    "    # Split the data on the best attribute\n",
    "    attribute_values = data[best_attribute].unique()\n",
    "    for value in attribute_values:\n",
    "        subset = data[data[best_attribute] == value]\n",
    "        if len(subset) == 0:\n",
    "            majority_class = data[target_attribute].mode()[0]\n",
    "            root.children[value] = TreeNode(label=majority_class)\n",
    "        else:\n",
    "            root.children[value] = id3_decision_tree(subset, attributes.drop(best_attribute), target_attribute, depth + 1, max_depth)\n",
    "\n",
    "    return root\n",
    "\n",
    "def select_best_attribute(data, attributes, target_attribute):\n",
    "    best_attribute = None\n",
    "    best_information_gain = -1\n",
    "\n",
    "    for attribute in attributes:\n",
    "        information_gain_value = information_gain(data, data[target_attribute], attribute)\n",
    "        if information_gain_value > best_information_gain:\n",
    "            best_information_gain = information_gain_value\n",
    "            best_attribute = attribute\n",
    "\n",
    "    return best_attribute\n",
    "\n",
    "def reduced_error_pruning(node, data, target):\n",
    "    if not node.children:\n",
    "        return\n",
    "\n",
    "    # Prune children nodes\n",
    "    for value, child_node in node.children.items():\n",
    "        reduced_error_pruning(child_node, data, target)\n",
    "\n",
    "    # Calculate error without pruning (using the original unpruned tree)\n",
    "    predictions_before_pruning = [predict(node, data_point) for data_point in data.to_numpy()]\n",
    "    error_before_pruning = np.sum(np.array(predictions_before_pruning) != np.array(target))\n",
    "\n",
    "    # Prune the current node\n",
    "    old_children = node.children.copy()\n",
    "    node.children = {}\n",
    "\n",
    "    # Calculate error after pruning (using the pruned tree)\n",
    "    predictions_after_pruning = [predict(node, data_point) for data_point in data.to_numpy()]\n",
    "    error_after_pruning = np.sum(np.array(predictions_after_pruning) != np.array(target))\n",
    "\n",
    "    # If pruning improves accuracy or doesn't change it, keep the pruning; otherwise, revert to old children\n",
    "    if error_after_pruning <= error_before_pruning:\n",
    "        print(f\"Pruned node: {node.attribute}, Depth: {node.depth}, Error before pruning: {error_before_pruning}, Error after pruning: {error_after_pruning}\")\n",
    "    else:\n",
    "        node.children = old_children\n",
    "\n",
    "# Define a function to predict the outcome for a single data point\n",
    "def predict(node, data_point):\n",
    "    if not node.children:\n",
    "        return node.label  # Return the label for leaf nodes\n",
    "\n",
    "    if node.attribute is None:\n",
    "        # This is a leaf node with a label\n",
    "        return node.label\n",
    "\n",
    "    value = data_point[node.attribute]\n",
    "\n",
    "    if value not in node.children:\n",
    "        print(f\"Warning: Value {value} not found in node {node.attribute}. Using majority class.\")\n",
    "        return np.argmax(np.bincount(node.label))\n",
    "\n",
    "    child_node = node.children[value]\n",
    "    return predict(child_node, data_point)\n",
    "\n",
    "# Define functions to calculate evaluation metrics\n",
    "def macro_precision(predictions, true_labels):\n",
    "    labels = np.unique(true_labels)\n",
    "    precision = []\n",
    "    for label in labels:\n",
    "        true_positive = np.sum((predictions == label) & (true_labels == label))\n",
    "        false_positive = np.sum((predictions == label) & (true_labels != label))\n",
    "        if true_positive + false_positive == 0:\n",
    "            precision.append(0)\n",
    "        else:\n",
    "            precision.append(true_positive / (true_positive + false_positive))\n",
    "    return np.mean(precision)\n",
    "\n",
    "def macro_recall(predictions, true_labels):\n",
    "    labels = np.unique(true_labels)\n",
    "    recall = []\n",
    "    for label in labels:\n",
    "        true_positive = np.sum((predictions == label) & (true_labels == label))\n",
    "        false_negative = np.sum((predictions != label) & (true_labels == label))\n",
    "        if true_positive + false_negative == 0:\n",
    "            recall.append(0)\n",
    "        else:\n",
    "            recall.append(true_positive / (true_positive + false_negative))\n",
    "    return np.mean(recall)\n",
    "\n",
    "# Define a function to print the pruned decision tree structure\n",
    "def print_pruned_tree(node, depth):\n",
    "    if node is None:\n",
    "        return\n",
    "    indent = \"  \" * depth\n",
    "    if node.attribute is not None:\n",
    "        print(f\"{indent}Attribute: {node.attribute}\")\n",
    "    if node.value is not None:\n",
    "        print(f\"{indent}Value: {node.value}\")\n",
    "    \n",
    "    if node.children:\n",
    "        for value, child_node in node.children.items():\n",
    "            # Check if the child node has a valid attribute\n",
    "            if child_node.attribute is not None:\n",
    "                print(f\"{indent}  If {child_node.attribute} == {value}:\")\n",
    "                print_pruned_tree(child_node, depth + 1)\n",
    "            else:\n",
    "                print(f\"{indent}  Predicted Outcome: {predict(child_node, pd.Series(node.data))}\")\n",
    "\n",
    "# Create a variable containing all columns other than \"Outcome\"\n",
    "attr = train_data.drop(columns=['Outcome'])\n",
    "\n",
    "# Build the ID3 decision tree\n",
    "tree = id3_decision_tree(train_data, attr.columns, 'Outcome', 0, 11)\n",
    "\n",
    "# Perform reduced error pruning on the tree\n",
    "reduced_error_pruning(tree, train_data, train_data['Outcome'])\n",
    "\n",
    "# Calculate train and test accuracies at different depths\n",
    "depths = list(range(1, 11))\n",
    "train_accuracies = []  # List to store training accuracies\n",
    "test_accuracies = []   # List to store test accuracies\n",
    "\n",
    "for depth in depths:\n",
    "    pruned_tree = id3_decision_tree(train_data, train_data.drop(columns=['Outcome']), 'Outcome', depth, max_depth=depth)\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    train_predictions = [predict(pruned_tree, train_data.iloc[i]) for i in range(len(train_data))]\n",
    "    train_accuracy = np.sum(train_predictions == train_data['Outcome']) / len(train_data)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    test_predictions = [predict(pruned_tree, test_data.iloc[i]) for i in range(len(test_data))]\n",
    "    test_accuracy = np.sum(test_predictions == test_data['Outcome']) / len(test_data)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "     # Print the accuracies\n",
    "    print(f\"Depth {depth}: Training Accuracy = {train_accuracy:.2f}, Test Accuracy = {test_accuracy:.2f}\")\n",
    "\n",
    "# Plot the variation in training and test accuracy with varying depths\n",
    "plt.plot(depths, train_accuracies, marker='o', label='Training Accuracy')\n",
    "plt.plot(depths, test_accuracies, marker='o', label='Test Accuracy')\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy vs. Tree Depth')\n",
    "plt.legend()  # Add legend to differentiate between training and test accuracy\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the pruned decision tree structure after pruning\n",
    "print(\"Pruned Decision Tree After Pruning:\")\n",
    "print_pruned_tree(tree, 0)\n",
    "\n",
    "# Make predictions using the pruned tree on the test data\n",
    "predictions = [predict(tree, test_data.iloc[i]) for i in range(len(test_data))]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = np.sum(predictions == test_data['Outcome']) / len(test_data)\n",
    "precision = macro_precision(predictions, test_data['Outcome'])\n",
    "recall = macro_recall(predictions, test_data['Outcome'])\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Macro Precision: {precision}\")\n",
    "print(f\"Macro Recall: {recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
